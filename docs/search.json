[
  {
    "objectID": "posts/slippery-attention/index.html",
    "href": "posts/slippery-attention/index.html",
    "title": "You don’t need to pay that much attention",
    "section": "",
    "text": "Main idea behind this lies behind one question: Why do we need to calculate attention scores for stuff that we already know?\nTo get what I mean, look at causal attention mask (or at least, how it looks like in Sheared LLaMA (Xia et al. 2024)):\n\n\n\n\n\n\n\n\nFigure 1: Causal Attention Mask\n\n\n\n\n\nEvery token can attend to every other token in the sequence. Pretty normal, right? After calculating attention scores, what is going on is that from this token model predicts the next token in the sequence, that is actually already known.\nLet’s consider the following sentence: “Six is afraid of seven” Let’s assume that our words are our tokens. Then, we have following sequence of tokens:\n&lt;s&gt;\nsix\nis\nafraid\nof\nseven\n\nFor some reason we predict that “is” comes after “six”, but we already know that “is” is the third token in the sequence. That’s the idea:\n\nWe don’t need to calculate attention scores for stuff that we already know, so we only need to calculate attention scores when we predict token that comes after the token last known\n\nIn this way, we skip the calculation of many attention scores. So, for a sequence with a length of 10, the attention mask should look like this:\n\n\n\n\n\n\n\n\nFigure 2: Incomplete Attention Mask\n\n\n\n\n\nLooks weird and looks like it shouldn’t work, and you’re right, it doesn’t work when made exactly like this.\n…\nThere’s a catch, though. It’s possible to make something like this work. First of all, there come attention sinks(Xiao et al. 2024). It’s special tokens (special because they emerge like that, not because they are designated to be special), that have greater impact on attention scores, and which are located at the very beginning of the sentence. We can add that:\n\n\n\n\n\n\n\n\nFigure 3: Incomplete attention with attention sinks\n\n\n\n\n\nThis one works better, but still not as good as I would like it to be. Adding something similar to sliding window attention does the trick and improves it by a very good margin:\n\n\n\n\n\n\n\n\nFigure 4: Slippery Attention\n\n\n\n\n\nToken at the very top is the one that we now make the predicton for: what comes after that. So for them, attention score is being calculated as usual. For others: just an attention sink, and token before current one.\nResults of this intervention are rather interesting: First of all, for example prompts that I use to evaluate how models are ruined by my interventions, not much changed. First example is “why did the chicken cross the road?” – outputs didn’t change at all. When I prompted it to write me “hello world” in C++, it still gave me results that we will consider correct, just in a different way.\nHere’s normal output:\n#include &lt;iostream&gt;\n\nint main()\n{\n    std::cout &lt;&lt; \"Hello world\";\n}\n\n\nA: You can use std::cout to print the string.\n#include &lt;iostream&gt;\n\nint main\nHere’s output with my intervention:\nHello world in C++\nint main() {\ncout &lt;&lt; \"Hello world in C++\" &lt;&lt; endl;\nreturn 0;\n}\n\n\nA: You can use std::cout to print the message.\nint main() {\nNot that big of a difference, isn’t it?\nMain difference seems to lie in class of tasks that require some kind of arithmetic, like directly asking to do sums or multiplications. These changed significantly:\nOriginal output:\nUser request: 10 * 2 equals what?\nAnswer : 10 * 2 = 20\nUser request: 10 * 2 equals what?\nAnswer : 10 * 2 = 200\nUser request: 10 * \noutput with my intervention:\nUser request: 10 * 2 equals what?\nAnswer : 10 * 2 = 20\n\nA: You can use modulo operator:\n&gt;&gt;&gt; 10 / 2 == 2\n\n\nA: You can use modulo operator:\nWith other prompts, modified model’s outputs are sometimes just stuff that has nothing to do with the request, which is not the case with the original model.\n\n\n\n\n\n\n\n\n\n\n\nTask\nMetric\nSlippery Attention\nVanilla\n\n\n\n\n0\nlambada_standard\nperplexity,none\n9889.734978\n9.064670\n\n\n1\nlambada_standard\nperplexity_stderr,none\n509.896498\n0.244574\n\n\n2\nlambada_standard\nacc,none\n0.023481\n0.521832\n\n\n3\nlambada_standard\nacc_stderr,none\n0.002110\n0.006959\n\n\n4\nxstorycloze_en\nacc,none\n0.518862\n0.691595\n\n\n5\nxstorycloze_en\nacc_stderr,none\n0.012858\n0.011885\n\n\n6\nlogiqa\nacc,none\n0.230415\n0.216590\n\n\n7\nlogiqa\nacc_stderr,none\n0.016517\n0.016157\n\n\n8\nlogiqa\nacc_norm,none\n0.274962\n0.273425\n\n\n9\nlogiqa\nacc_norm_stderr,none\n0.017513\n0.017482\n\n\n\n\n\n\n\nFigure 5: Benchmark results for Sheared Llama with Slippery Attention\n\n\n\n\nAs we can see, results are mixed. The one that is the most confusing is anything that calculates perplexity. Since every already known token calculates it’s attention score is calculated mostly in isolation from other tokens, we get poor perplexity scores. Other metrics are weird too. In some cases, we even got improvements (like for xstoryclose_en, logiqa). On others, like lambada, our scores dipped rather deep. Basically, we can only properly benchmark this intervention on tasks that require us to generate tokens, instead of just calculating perplexity (it will ALWAYS be poor because of how this thing works).\nGood thing is that model didn’t degrade beyond recognition even by slippery attention of triangular shape, and we’re not limited to this triangle-shape attention mask, by the way, it’s just a way to show that we can have some kind of sparsity in our attention pattern for prefill, and full attention for tokens that are generated. Something similar to this can be found in SampleAttention(Zhu et al. 2024), but I didn’t have time yet to properly take a look at it. Main difference slippery attention has from other sparsity patterns is that shape of the pattern changes depending on what tokens are: inputs known beforehand or if they’re tokens generated as a model response.\nIn it’s simpliest form, this intervention is implemented as another module that wraps the original self attention block, and only use the slippery attention when input sequence is longer than one. Code will be added to this blogpost once I clean it up.\nThis “structured sparsity”/“slippery attention” gives us way more leeway in attention mask design (and makes entire thing sub-quadratic!), with which, new possibilities open up, which I will discuss in one of the next posts.\n\n\n@misc{avietisov2024dont,\n  title={You don't need to pay that much attention},\n  author={Hlib Avietisov},\n  year={2024},\n  howpublished={Online, available at \\url{https://havietisov.github.io/posts/slippery-attention/}},\n}"
  },
  {
    "objectID": "posts/slippery-attention/index.html#slippery-attention",
    "href": "posts/slippery-attention/index.html#slippery-attention",
    "title": "You don’t need to pay that much attention",
    "section": "",
    "text": "Main idea behind this lies behind one question: Why do we need to calculate attention scores for stuff that we already know?\nTo get what I mean, look at causal attention mask (or at least, how it looks like in Sheared LLaMA (Xia et al. 2024)):\n\n\n\n\n\n\n\n\nFigure 1: Causal Attention Mask\n\n\n\n\n\nEvery token can attend to every other token in the sequence. Pretty normal, right? After calculating attention scores, what is going on is that from this token model predicts the next token in the sequence, that is actually already known.\nLet’s consider the following sentence: “Six is afraid of seven” Let’s assume that our words are our tokens. Then, we have following sequence of tokens:\n&lt;s&gt;\nsix\nis\nafraid\nof\nseven\n\nFor some reason we predict that “is” comes after “six”, but we already know that “is” is the third token in the sequence. That’s the idea:\n\nWe don’t need to calculate attention scores for stuff that we already know, so we only need to calculate attention scores when we predict token that comes after the token last known\n\nIn this way, we skip the calculation of many attention scores. So, for a sequence with a length of 10, the attention mask should look like this:\n\n\n\n\n\n\n\n\nFigure 2: Incomplete Attention Mask\n\n\n\n\n\nLooks weird and looks like it shouldn’t work, and you’re right, it doesn’t work when made exactly like this.\n…\nThere’s a catch, though. It’s possible to make something like this work. First of all, there come attention sinks(Xiao et al. 2024). It’s special tokens (special because they emerge like that, not because they are designated to be special), that have greater impact on attention scores, and which are located at the very beginning of the sentence. We can add that:\n\n\n\n\n\n\n\n\nFigure 3: Incomplete attention with attention sinks\n\n\n\n\n\nThis one works better, but still not as good as I would like it to be. Adding something similar to sliding window attention does the trick and improves it by a very good margin:\n\n\n\n\n\n\n\n\nFigure 4: Slippery Attention\n\n\n\n\n\nToken at the very top is the one that we now make the predicton for: what comes after that. So for them, attention score is being calculated as usual. For others: just an attention sink, and token before current one.\nResults of this intervention are rather interesting: First of all, for example prompts that I use to evaluate how models are ruined by my interventions, not much changed. First example is “why did the chicken cross the road?” – outputs didn’t change at all. When I prompted it to write me “hello world” in C++, it still gave me results that we will consider correct, just in a different way.\nHere’s normal output:\n#include &lt;iostream&gt;\n\nint main()\n{\n    std::cout &lt;&lt; \"Hello world\";\n}\n\n\nA: You can use std::cout to print the string.\n#include &lt;iostream&gt;\n\nint main\nHere’s output with my intervention:\nHello world in C++\nint main() {\ncout &lt;&lt; \"Hello world in C++\" &lt;&lt; endl;\nreturn 0;\n}\n\n\nA: You can use std::cout to print the message.\nint main() {\nNot that big of a difference, isn’t it?\nMain difference seems to lie in class of tasks that require some kind of arithmetic, like directly asking to do sums or multiplications. These changed significantly:\nOriginal output:\nUser request: 10 * 2 equals what?\nAnswer : 10 * 2 = 20\nUser request: 10 * 2 equals what?\nAnswer : 10 * 2 = 200\nUser request: 10 * \noutput with my intervention:\nUser request: 10 * 2 equals what?\nAnswer : 10 * 2 = 20\n\nA: You can use modulo operator:\n&gt;&gt;&gt; 10 / 2 == 2\n\n\nA: You can use modulo operator:\nWith other prompts, modified model’s outputs are sometimes just stuff that has nothing to do with the request, which is not the case with the original model.\n\n\n\n\n\n\n\n\n\n\n\nTask\nMetric\nSlippery Attention\nVanilla\n\n\n\n\n0\nlambada_standard\nperplexity,none\n9889.734978\n9.064670\n\n\n1\nlambada_standard\nperplexity_stderr,none\n509.896498\n0.244574\n\n\n2\nlambada_standard\nacc,none\n0.023481\n0.521832\n\n\n3\nlambada_standard\nacc_stderr,none\n0.002110\n0.006959\n\n\n4\nxstorycloze_en\nacc,none\n0.518862\n0.691595\n\n\n5\nxstorycloze_en\nacc_stderr,none\n0.012858\n0.011885\n\n\n6\nlogiqa\nacc,none\n0.230415\n0.216590\n\n\n7\nlogiqa\nacc_stderr,none\n0.016517\n0.016157\n\n\n8\nlogiqa\nacc_norm,none\n0.274962\n0.273425\n\n\n9\nlogiqa\nacc_norm_stderr,none\n0.017513\n0.017482\n\n\n\n\n\n\n\nFigure 5: Benchmark results for Sheared Llama with Slippery Attention\n\n\n\n\nAs we can see, results are mixed. The one that is the most confusing is anything that calculates perplexity. Since every already known token calculates it’s attention score is calculated mostly in isolation from other tokens, we get poor perplexity scores. Other metrics are weird too. In some cases, we even got improvements (like for xstoryclose_en, logiqa). On others, like lambada, our scores dipped rather deep. Basically, we can only properly benchmark this intervention on tasks that require us to generate tokens, instead of just calculating perplexity (it will ALWAYS be poor because of how this thing works).\nGood thing is that model didn’t degrade beyond recognition even by slippery attention of triangular shape, and we’re not limited to this triangle-shape attention mask, by the way, it’s just a way to show that we can have some kind of sparsity in our attention pattern for prefill, and full attention for tokens that are generated. Something similar to this can be found in SampleAttention(Zhu et al. 2024), but I didn’t have time yet to properly take a look at it. Main difference slippery attention has from other sparsity patterns is that shape of the pattern changes depending on what tokens are: inputs known beforehand or if they’re tokens generated as a model response.\nIn it’s simpliest form, this intervention is implemented as another module that wraps the original self attention block, and only use the slippery attention when input sequence is longer than one. Code will be added to this blogpost once I clean it up.\nThis “structured sparsity”/“slippery attention” gives us way more leeway in attention mask design (and makes entire thing sub-quadratic!), with which, new possibilities open up, which I will discuss in one of the next posts.\n\n\n@misc{avietisov2024dont,\n  title={You don't need to pay that much attention},\n  author={Hlib Avietisov},\n  year={2024},\n  howpublished={Online, available at \\url{https://havietisov.github.io/posts/slippery-attention/}},\n}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Blog about LLMs and inference time optimizations for them. Some images are generated by Dall-E"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adventures in Inference Time Optimizations (and cool stuff)",
    "section": "",
    "text": "You don’t need to pay that much attention\n\n\n\n\n\n\nmachine-learning\n\n\ntransformers\n\n\nattention\n\n\ncode\n\n\noptimization\n\n\n\nA sub-quadratic attention trick to enchance even pre-trained LLMs with speed and incredible abilities\n\n\n\n\n\nSep 16, 2024\n\n\nHlib Avietisov\n\n\n\n\n\n\nNo matching items"
  }
]