[
  {
    "objectID": "posts/slippery-attention/index.html",
    "href": "posts/slippery-attention/index.html",
    "title": "You don’t need to pay that much attention",
    "section": "",
    "text": "Main idea behind this lies behind one question: Why do we need to calculate attention scores for stuff that we already know?\nTo get what I mean, look at causal attention mask (or at least, how it looks like in Sheared LLaMA (Xia et al. 2024)):\n\n\n\n\n\n\n\n\nFigure 1: Causal Attention Mask\n\n\n\n\n\nEvery token can attend to every other token in the sequence. Pretty normal, right? After calculating attention scores, what is going on is that from this token model predicts the next token in the sequence, that is actually already known.\nLet’s consider the following sentence: “Six is afraid of seven” Let’s assume that our words are our tokens. Then, we have following sequence of tokens:\n&lt;s&gt;\nsix\nis\nafraid\nof\nseven\n\nFor some reason we predict that “is” comes after “six”, but we already know that “is” is the third token in the sequence. That’s the idea:\n\nWe don’t need to calculate attention scores for stuff that we already know, so we only need to calculate attention scores when we predict token that comes after the token last known\n\nIn this way, we skip the calculation of many attention scores. So, for a sequence with a length of 10, the attention mask should look like this:\n\n\n\n\n\n\n\n\nFigure 2: Incomplete Attention Mask\n\n\n\n\n\nLooks weird and looks like it shouldn’t work, and you’re right, it doesn’t work when made exactly like this.\n…\nThere’s a catch, though. It’s possible to make something like this work. First of all, there come attention sinks(Xiao et al. 2024). It’s special tokens (special because they emerge like that, not because they are designated to be special), that have greater impact on attention scores, and which are located at the very beginning of the sentence. We can add that:\n\n\n\n\n\n\n\n\nFigure 3: Incomplete attention with attention sinks\n\n\n\n\n\nThis one works better, but still not as good as I would like it to be. Adding something similar to sliding window attention does the trick and improves it by a very good margin:\n\n\n\n\n\n\n\n\nFigure 4: Slippery Attention\n\n\n\n\n\nToken at the very top is the one that we now make the predicton for: what comes after that. So for them, attention score is being calculated as usual. For others: just an attention sink, and token before current one.\nResults of this intervention are rather interesting: First of all, for example prompts that I use to evaluate how models are ruined by my interventions, not much changed. First example is “why did the chicken cross the road?” – outputs didn’t change at all. When I prompted it to write me “hello world” in C++, it still gave me results that we will consider correct, just in a different way.\nHere’s normal output:\n#include &lt;iostream&gt;\n\nint main()\n{\n    std::cout &lt;&lt; \"Hello world\";\n}\n\n\nA: You can use std::cout to print the string.\n#include &lt;iostream&gt;\n\nint main\nHere’s output with my intervention:\nHello world in C++\nint main() {\ncout &lt;&lt; \"Hello world in C++\" &lt;&lt; endl;\nreturn 0;\n}\n\n\nA: You can use std::cout to print the message.\nint main() {\nNot that big of a difference, isn’t it?\nMain difference seems to lie in class of tasks that require some kind of arithmetic, like directly asking to do sums or multiplications. These changed significantly:\nOriginal output:\nUser request: 10 * 2 equals what?\nAnswer : 10 * 2 = 20\nUser request: 10 * 2 equals what?\nAnswer : 10 * 2 = 200\nUser request: 10 * \noutput with my intervention:\nUser request: 10 * 2 equals what?\nAnswer : 10 * 2 = 20\n\nA: You can use modulo operator:\n&gt;&gt;&gt; 10 / 2 == 2\n\n\nA: You can use modulo operator:\nWith other prompts, modified model’s outputs are sometimes just stuff that has nothing to do with the request, which is not the case with the original model.\n\n\n\n\n\n\n\n\n\n\n\nTask\nMetric\nSlippery Attention\nVanilla\n\n\n\n\n0\nlambada_standard\nperplexity,none\n9889.734978\n9.064670\n\n\n1\nlambada_standard\nperplexity_stderr,none\n509.896498\n0.244574\n\n\n2\nlambada_standard\nacc,none\n0.023481\n0.521832\n\n\n3\nlambada_standard\nacc_stderr,none\n0.002110\n0.006959\n\n\n4\nxstorycloze_en\nacc,none\n0.518862\n0.691595\n\n\n5\nxstorycloze_en\nacc_stderr,none\n0.012858\n0.011885\n\n\n6\nlogiqa\nacc,none\n0.230415\n0.216590\n\n\n7\nlogiqa\nacc_stderr,none\n0.016517\n0.016157\n\n\n8\nlogiqa\nacc_norm,none\n0.274962\n0.273425\n\n\n9\nlogiqa\nacc_norm_stderr,none\n0.017513\n0.017482\n\n\n\n\n\n\n\nFigure 5: Benchmark results for Sheared Llama with Slippery Attention\n\n\n\n\nAs we can see, results are mixed. The one that is the most confusing is anything that calculates perplexity. Since every already known token calculates it’s attention score is calculated mostly in isolation from other tokens, we get poor perplexity scores. Other metrics are weird too. In some cases, we even got improvements (like for xstoryclose_en, logiqa). On others, like lambada, our scores dipped rather deep. Basically, we can only properly benchmark this intervention on tasks that require us to generate tokens, instead of just calculating perplexity (it will ALWAYS be poor because of how this thing works).\nGood thing is that model didn’t degrade beyond recognition even by slippery attention of triangular shape, and we’re not limited to this triangle-shape attention mask, by the way, it’s just a way to show that we can have some kind of sparsity in our attention pattern for prefill, and full attention for tokens that are generated. Something similar to this can be found in SampleAttention(Zhu et al. 2024), but I didn’t have time yet to properly take a look at it. Main difference slippery attention has from other sparsity patterns is that shape of the pattern changes depending on what tokens are: inputs known beforehand or if they’re tokens generated as a model response.\nIn it’s simpliest form, this intervention is implemented as another module that wraps the original self attention block, and only use the slippery attention when input sequence is longer than one. Code will be added to this blogpost once I clean it up.\nThis “structured sparsity”/“slippery attention” gives us way more leeway in attention mask design (and makes entire thing sub-quadratic!), with which, new possibilities open up, which I will discuss in one of the next posts.\n\n\n@misc{avietisov2024dont,\n  title={You don't need to pay that much attention},\n  author={Hlib Avietisov},\n  year={2024},\n  howpublished={Online, available at \\url{https://havietisov.github.io/posts/slippery-attention/}},\n}"
  },
  {
    "objectID": "posts/slippery-attention/index.html#slippery-attention",
    "href": "posts/slippery-attention/index.html#slippery-attention",
    "title": "You don’t need to pay that much attention",
    "section": "",
    "text": "Main idea behind this lies behind one question: Why do we need to calculate attention scores for stuff that we already know?\nTo get what I mean, look at causal attention mask (or at least, how it looks like in Sheared LLaMA (Xia et al. 2024)):\n\n\n\n\n\n\n\n\nFigure 1: Causal Attention Mask\n\n\n\n\n\nEvery token can attend to every other token in the sequence. Pretty normal, right? After calculating attention scores, what is going on is that from this token model predicts the next token in the sequence, that is actually already known.\nLet’s consider the following sentence: “Six is afraid of seven” Let’s assume that our words are our tokens. Then, we have following sequence of tokens:\n&lt;s&gt;\nsix\nis\nafraid\nof\nseven\n\nFor some reason we predict that “is” comes after “six”, but we already know that “is” is the third token in the sequence. That’s the idea:\n\nWe don’t need to calculate attention scores for stuff that we already know, so we only need to calculate attention scores when we predict token that comes after the token last known\n\nIn this way, we skip the calculation of many attention scores. So, for a sequence with a length of 10, the attention mask should look like this:\n\n\n\n\n\n\n\n\nFigure 2: Incomplete Attention Mask\n\n\n\n\n\nLooks weird and looks like it shouldn’t work, and you’re right, it doesn’t work when made exactly like this.\n…\nThere’s a catch, though. It’s possible to make something like this work. First of all, there come attention sinks(Xiao et al. 2024). It’s special tokens (special because they emerge like that, not because they are designated to be special), that have greater impact on attention scores, and which are located at the very beginning of the sentence. We can add that:\n\n\n\n\n\n\n\n\nFigure 3: Incomplete attention with attention sinks\n\n\n\n\n\nThis one works better, but still not as good as I would like it to be. Adding something similar to sliding window attention does the trick and improves it by a very good margin:\n\n\n\n\n\n\n\n\nFigure 4: Slippery Attention\n\n\n\n\n\nToken at the very top is the one that we now make the predicton for: what comes after that. So for them, attention score is being calculated as usual. For others: just an attention sink, and token before current one.\nResults of this intervention are rather interesting: First of all, for example prompts that I use to evaluate how models are ruined by my interventions, not much changed. First example is “why did the chicken cross the road?” – outputs didn’t change at all. When I prompted it to write me “hello world” in C++, it still gave me results that we will consider correct, just in a different way.\nHere’s normal output:\n#include &lt;iostream&gt;\n\nint main()\n{\n    std::cout &lt;&lt; \"Hello world\";\n}\n\n\nA: You can use std::cout to print the string.\n#include &lt;iostream&gt;\n\nint main\nHere’s output with my intervention:\nHello world in C++\nint main() {\ncout &lt;&lt; \"Hello world in C++\" &lt;&lt; endl;\nreturn 0;\n}\n\n\nA: You can use std::cout to print the message.\nint main() {\nNot that big of a difference, isn’t it?\nMain difference seems to lie in class of tasks that require some kind of arithmetic, like directly asking to do sums or multiplications. These changed significantly:\nOriginal output:\nUser request: 10 * 2 equals what?\nAnswer : 10 * 2 = 20\nUser request: 10 * 2 equals what?\nAnswer : 10 * 2 = 200\nUser request: 10 * \noutput with my intervention:\nUser request: 10 * 2 equals what?\nAnswer : 10 * 2 = 20\n\nA: You can use modulo operator:\n&gt;&gt;&gt; 10 / 2 == 2\n\n\nA: You can use modulo operator:\nWith other prompts, modified model’s outputs are sometimes just stuff that has nothing to do with the request, which is not the case with the original model.\n\n\n\n\n\n\n\n\n\n\n\nTask\nMetric\nSlippery Attention\nVanilla\n\n\n\n\n0\nlambada_standard\nperplexity,none\n9889.734978\n9.064670\n\n\n1\nlambada_standard\nperplexity_stderr,none\n509.896498\n0.244574\n\n\n2\nlambada_standard\nacc,none\n0.023481\n0.521832\n\n\n3\nlambada_standard\nacc_stderr,none\n0.002110\n0.006959\n\n\n4\nxstorycloze_en\nacc,none\n0.518862\n0.691595\n\n\n5\nxstorycloze_en\nacc_stderr,none\n0.012858\n0.011885\n\n\n6\nlogiqa\nacc,none\n0.230415\n0.216590\n\n\n7\nlogiqa\nacc_stderr,none\n0.016517\n0.016157\n\n\n8\nlogiqa\nacc_norm,none\n0.274962\n0.273425\n\n\n9\nlogiqa\nacc_norm_stderr,none\n0.017513\n0.017482\n\n\n\n\n\n\n\nFigure 5: Benchmark results for Sheared Llama with Slippery Attention\n\n\n\n\nAs we can see, results are mixed. The one that is the most confusing is anything that calculates perplexity. Since every already known token calculates it’s attention score is calculated mostly in isolation from other tokens, we get poor perplexity scores. Other metrics are weird too. In some cases, we even got improvements (like for xstoryclose_en, logiqa). On others, like lambada, our scores dipped rather deep. Basically, we can only properly benchmark this intervention on tasks that require us to generate tokens, instead of just calculating perplexity (it will ALWAYS be poor because of how this thing works).\nGood thing is that model didn’t degrade beyond recognition even by slippery attention of triangular shape, and we’re not limited to this triangle-shape attention mask, by the way, it’s just a way to show that we can have some kind of sparsity in our attention pattern for prefill, and full attention for tokens that are generated. Something similar to this can be found in SampleAttention(Zhu et al. 2024), but I didn’t have time yet to properly take a look at it. Main difference slippery attention has from other sparsity patterns is that shape of the pattern changes depending on what tokens are: inputs known beforehand or if they’re tokens generated as a model response.\nIn it’s simpliest form, this intervention is implemented as another module that wraps the original self attention block, and only use the slippery attention when input sequence is longer than one. Code will be added to this blogpost once I clean it up.\nThis “structured sparsity”/“slippery attention” gives us way more leeway in attention mask design (and makes entire thing sub-quadratic!), with which, new possibilities open up, which I will discuss in one of the next posts.\n\n\n@misc{avietisov2024dont,\n  title={You don't need to pay that much attention},\n  author={Hlib Avietisov},\n  year={2024},\n  howpublished={Online, available at \\url{https://havietisov.github.io/posts/slippery-attention/}},\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adventures in Inference Time Optimizations (and cool stuff)",
    "section": "",
    "text": "Unnatural abilities of context stitching\n\n\n\n\n\n\nmachine-learning\n\n\ntransformers\n\n\nattention\n\n\ncode\n\n\noptimization\n\n\n\nAs was described in the “You don’t need to pay that much attention” post, from slippery attention’s property to have different attention patterns for different classes of tokens, coming a rather powerful technique, which I called “Context stitching”, which allows to put context together from precomputed chunks like lego blocks. In this blog post, I elaborate how.\n\n\n\n\n\nSep 24, 2024\n\n\nHlib Avietisov\n\n\n\n\n\n\n\n\n\n\n\n\nYou don’t need to pay that much attention\n\n\n\n\n\n\nmachine-learning\n\n\ntransformers\n\n\nattention\n\n\ncode\n\n\noptimization\n\n\n\nA sub-quadratic attention trick to enchance even pre-trained LLMs with speed and incredible abilities\n\n\n\n\n\nSep 16, 2024\n\n\nHlib Avietisov\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Blog about LLMs and inference time optimizations for them. Some images are generated by Dall-E"
  },
  {
    "objectID": "posts/context-stitching/index.html",
    "href": "posts/context-stitching/index.html",
    "title": "Unnatural abilities of context stitching",
    "section": "",
    "text": "Core idea of context stitching is precomputing of KV values for different chunks of text, then to be loaded at inference time when required, and put together to form final context. Problem for this is assumed interdependence of chunks, since in vanilla attention, KV values for next chunk of text depends on previous chunk, which should create a bit of a problem."
  },
  {
    "objectID": "posts/context-stitching/index.html#idea",
    "href": "posts/context-stitching/index.html#idea",
    "title": "Unnatural abilities of context stitching",
    "section": "",
    "text": "Core idea of context stitching is precomputing of KV values for different chunks of text, then to be loaded at inference time when required, and put together to form final context. Problem for this is assumed interdependence of chunks, since in vanilla attention, KV values for next chunk of text depends on previous chunk, which should create a bit of a problem."
  },
  {
    "objectID": "posts/context-stitching/index.html#recap-of-slippery-attention",
    "href": "posts/context-stitching/index.html#recap-of-slippery-attention",
    "title": "Unnatural abilities of context stitching",
    "section": "Recap of slippery attention",
    "text": "Recap of slippery attention\nIn the previous post, “You don’t need to pay that much attention”, I described rather interesting property of attention mechanism to not break generation of text, if we replace vanilla attention pattern for already known tokens, with new sparse attention pattern, which shows that unless we need to predict a new token, we can actually skip a lot of computations.\nNaturally, this is what enables the context stitching, since now we prove that KV values are mostly independent for chunks of text when there’s no new predictions to make.\nIn the original post, I’ve made an example of slippery attention with triangular attention pattern, but we’re actually not limited to that. It was just a display that even with such drastic changes in the attention pattern, LLM still remains capable of many tasks, with severe degradation only on some tasks."
  },
  {
    "objectID": "posts/context-stitching/index.html#context-stitching",
    "href": "posts/context-stitching/index.html#context-stitching",
    "title": "Unnatural abilities of context stitching",
    "section": "Context stitching",
    "text": "Context stitching\nWhat now we can do, is to take advantage of this property by precomputing KV values with normal attention pattern in “precompute time”, and then, at inference time, load these KV values into memory, to make predictions. As was shown in “You don’t need to pay that much attention” post, we will need to have full attention mask only for newly generated token.\nLast problem for context stiching remains is positioning of these chunks in the context. For that, I will abuse property of a transformer to not care about continuity of positional embeddings. Basically, we compute KV values for each chunk starting at predefined positions, after which, context will look like it has multiple chunks decently spaced out from each other, with new tokens generated at a decent distance from last predefined token. That’s the main limitation of this, model should be taught to handle big contexts, as much as you need. Then, it’s only a matter of figuring out where you would want to put your chunks inside the context, and then, precompute KV values.\nTheoretically, for prefill, we would need to load KV values for system prompt from memory/storage, compute user request KV values using slippery attention (with triangular mask, to save on compute), and only then predict new token with full mask. As an example, let’s look at following sequence:\n\n&lt;s&gt;Use following table to help you answer the question:\n2+2=4\n2+3=5\nQuestion: 2+2=\nAnswer:\n\nDifferent colors of text are used to distinguish between different chunks of text and their origin. Blue and green are different chunks of text, loaded from storage, magenta comes for system prompt, red is for user request.\nSo, red is calculated with triangular mask, rest gets precomputed with with local full-attention mask. So, for sequence above, let’s build an (approximate) attention mask to show what I mean:\n\n\n\n\n\n\n\n\nFigure 1: Stitched Causal Attention Mask\n\n\n\n\n\nNotice how last token, attributed to system prompt, has attention to entire context. This is required to properly predict next token. All following generated tokens will attend to everything in the context, but note how system prompt, and chunks, all can have precomputed KV values since they don’t attend to each other. With little exception of token at index 0, which is also called “attention sink”(Xiao et al. 2024), but that can also be precomputed, since we always know what token it would be: it’s a sequence start token.\nYou might find it a bit similar to what’s happening in [Quiet-STaR](Zelikman et al. 2024) paper, where they also precompute KV values for different chunks of text, but it’s done with motivation of running inference of different tokens in parallel. Here, however, motivation is precomputation of KV values to save time during inference. In example above, in order to get 4, we only need to compute KV values for single token, which will be just 4. Rest is just loaded. 1 vs (25*25)/2, isn’t it neat?\nFor now, this is just an idea, there’s no implementation YET. Will have to do it a bit later once I finish with other stuff.\n\nCite this as\n@misc{avietisov2024contextstitching,\n  title={Unnatural abilities of context stitching},\n  author={Hlib Avietisov},\n  year={2024},\n  howpublished={Online, available at \\url{https://havietisov.github.io/posts/context-stitching/}},\n}"
  }
]